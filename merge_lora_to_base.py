from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch
import os

# ======== é…ç½® ========
BASE_MODEL = "Qwen/Qwen2.5-1.5B-Instruct"
LORA_DIR = "./chat_model_lora"
OUTPUT_DIR = "./chat_model_merged"

# ======== æ£€æŸ¥è·¯å¾„ ========
if not os.path.exists(LORA_DIR):
    raise FileNotFoundError(f"âŒ LoRA æ–‡ä»¶å¤¹æœªæ‰¾åˆ°: {LORA_DIR}")

print("ğŸ”¹ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹...")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map="auto"
)

print("ğŸ”¹ æ­£åœ¨åŠ è½½ LoRA é€‚é…å™¨...")
model = PeftModel.from_pretrained(base_model, LORA_DIR)

print("ğŸ”¹ å¼€å§‹åˆå¹¶ LoRA æƒé‡ï¼ˆè¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼‰...")
merged_model = model.merge_and_unload()
merged_model.save_pretrained(OUTPUT_DIR)
print(f"âœ… åˆå¹¶å®Œæˆï¼æ–°æ¨¡å‹å·²ä¿å­˜è‡³: {OUTPUT_DIR}")

# ä¿å­˜ tokenizer
print("ğŸ’¾ æ­£åœ¨ä¿å­˜ tokenizer ...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
tokenizer.save_pretrained(OUTPUT_DIR)
print("âœ… Tokenizer ä¿å­˜æˆåŠŸã€‚")
