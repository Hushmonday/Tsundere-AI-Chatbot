import sys
import os
import types
import platform
import io
import contextlib
import torch
import gradio as gr
from transformers import AutoTokenizer, AutoModelForCausalLM

# =========================================================
# ğŸš« ä¼ªé€  bitsandbytes æ¨¡å—ï¼Œé˜²æ­¢ Windows CUDA æŠ¥é”™
# =========================================================
fake_bnb = types.ModuleType("bitsandbytes")
fake_bnb.__spec__ = types.SimpleNamespace()
fake_bnb.__file__ = "fake_bitsandbytes.py"
fake_bnb.__path__ = []
fake_bnb.nn = types.SimpleNamespace(modules=types.SimpleNamespace(Linear8bitLt=None), Linear8bitLt=None)
fake_bnb.cuda_setup = types.SimpleNamespace(main=lambda: None)
sys.modules["bitsandbytes"] = fake_bnb

# =========================================================
# ğŸŒ ç¯å¢ƒè®¾ç½®
# =========================================================
os.environ["BITSANDBYTES_NOWELCOME"] = "1"
os.environ["BITSANDBYTES_DISABLE"] = "1"
os.environ["PEFT_BACKEND"] = "TORCH"
os.environ["USE_TORCH_FOR_LORA"] = "1"

system = platform.system()
print(f"ğŸ–¥ï¸ å½“å‰ç³»ç»Ÿï¼š{system}")
if system == "Windows":
    print("âš ï¸ è‡ªåŠ¨ç¦ç”¨ bitsandbytesï¼ˆWindows ä¸æ”¯æŒ CUDA DLLï¼‰")
else:
    print("âœ… Linux / macOS å¯ä½¿ç”¨é‡åŒ–")

# =========================================================
# ğŸ“¦ æ¨¡å‹è·¯å¾„é…ç½®
# =========================================================
BASE_MODEL = "./chat_model_merged"      # âœ… å·²åˆå¹¶å¥½çš„æ¨¡å‹
FINETUNED_MODEL = "./chat_model_lora"   # è‹¥æ—  LoRAï¼Œå¯è®¾ä¸º None

# =========================================================
# ğŸ§  åŠ è½½ Tokenizer
# =========================================================
print("ğŸ”¹ æ­£åœ¨åŠ è½½ tokenizer ...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)

if not getattr(tokenizer, "chat_template", None):
    print("âš ï¸ chat_template ä¸¢å¤±ï¼Œè‡ªåŠ¨è¡¥å……é»˜è®¤æ¨¡æ¿ã€‚")
    tokenizer.chat_template = (
        "{% for message in messages %}"
        "{{ '<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>\n' }}"
        "{% endfor %}"
        "{% if add_generation_prompt %}<|im_start|>assistant\n{% endif %}"
    )

# =========================================================
# âš™ï¸ åŠ è½½æ¨¡å‹
# =========================================================
print("ğŸ”¹ æ­£åœ¨åŠ è½½åŸºç¡€æ¨¡å‹ ...")
base_model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto",
    low_cpu_mem_usage=True
)
print("âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")

# =========================================================
# ğŸ”§ å°è¯•åŠ è½½ LoRAï¼ˆè‹¥å­˜åœ¨ï¼‰
# =========================================================
if FINETUNED_MODEL is not None and os.path.exists(FINETUNED_MODEL):
    print("ğŸ”¹ æ£€æµ‹åˆ° LoRA æ¨¡å‹ï¼Œæ­£åœ¨åŠ è½½å¹¶åˆå¹¶ ...")
    try:
        from peft import PeftModel
        model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)
        model = model.merge_and_unload()
        print("âœ… LoRA æƒé‡å·²æˆåŠŸåˆå¹¶ï¼")
    except Exception as e:
        print(f"âš ï¸ LoRA åŠ è½½å¤±è´¥ï¼Œå°†ç»§ç»­ä½¿ç”¨åŸºç¡€æ¨¡å‹: {e}")
        model = base_model
else:
    print("ğŸ§© æœªæŒ‡å®š LoRAï¼Œç›´æ¥ä½¿ç”¨åˆå¹¶æ¨¡å‹ã€‚")
    model = base_model

# =========================================================
# ğŸ§© Python ä»£ç æ‰§è¡Œæ¨¡å—
# =========================================================
def execute_python(code):
    """å®‰å…¨æ‰§è¡Œ Python ä»£ç ï¼Œå¹¶æ•è·è¾“å‡º"""
    try:
        buffer = io.StringIO()
        with contextlib.redirect_stdout(buffer):
            exec(code, {})
        output = buffer.getvalue()
        if not output.strip():
            output = "âœ… ä»£ç æ‰§è¡ŒæˆåŠŸï¼ˆæ— è¾“å‡ºï¼‰"
        return output
    except Exception as e:
        return f"âš ï¸ æ‰§è¡Œå‡ºé”™ï¼š{e}"

# =========================================================
# ğŸ’¬ èŠå¤©é€»è¾‘ï¼šå¯è‡ªç”±ç”Ÿæˆ + è‡ªåŠ¨æ£€æµ‹ä»£ç 
# =========================================================
def chat_fn(message, history):
    try:
        messages = []
        for user_msg, bot_msg in history:
            messages.append({"role": "user", "content": user_msg})
            if bot_msg:
                messages.append({"role": "assistant", "content": bot_msg})
        messages.append({"role": "user", "content": message})

        # === Step 0. é¢å¤–ç³»ç»Ÿæç¤ºè¯ï¼šå¼ºåˆ¶å¯å‘æ¨¡å‹å†™ä»£ç  ===
        sys_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¼šå†™Pythonä»£ç çš„AIåŠ©æ‰‹ã€‚å¦‚æœç”¨æˆ·è¯·æ±‚'å†™ä»£ç 'ã€'å¸®æˆ‘å†™Pythonå‡½æ•°'æˆ–ç±»ä¼¼å†…å®¹ï¼Œ"
            "è¯·è¿”å›å¸¦æœ‰ ```python ... ``` çš„å®Œæ•´ä»£ç å—ï¼Œä¸è¦çœç•¥ã€‚"
            "ç”Ÿæˆåï¼Œä»£ç å°†ä¼šè¢«è‡ªåŠ¨æ‰§è¡Œï¼Œè¯·ç¡®ä¿èƒ½ç›´æ¥è¿è¡Œã€‚"
        )
        messages.insert(0, {"role": "system", "content": sys_prompt})

        # ç”Ÿæˆ prompt
        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        # ======= Step 1. æ¨¡å‹ç”Ÿæˆ =======
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512,
                temperature=0.7,  # é™ä½éšæœºæ€§ï¼Œè®©è¾“å‡ºæ›´åƒä»£ç 
                top_p=0.9,
                repetition_penalty=1.05,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = text[len(prompt):].strip() if prompt else text.strip()

        # ======= Step 2. æ£€æµ‹æ˜¯å¦åŒ…å«Pythonä»£ç  =======
        msg_lower = message.lower()
        if "python" in msg_lower or "ä»£ç " in message:
            code_start = answer.find("```python")
            code_end = answer.find("```", code_start + 9)
            if code_start != -1 and code_end != -1:
                code = answer[code_start + 9:code_end].strip()
                result = execute_python(code)
                answer += f"\n\nğŸ§  æ‰§è¡Œç»“æœï¼š\n{result}"
            else:
                answer += "\n\nâš ï¸ æ²¡æ£€æµ‹åˆ°å¯æ‰§è¡Œçš„Pythonä»£ç å—ï¼Œè¯·é‡è¯•ã€‚"

        # ======= Step 3. å‚²å¨‡æ¶¦è‰² =======
        if any(x in msg_lower for x in ["ä½ å¥½", "hi", "hello", "hey"]):
            answer = f"æ‰ã€æ‰ä¸æ˜¯ç‰¹åœ°æƒ³ç†ä½ å‘¢â€¦â€¦ä½ å¥½å‘€ï¼ğŸ˜¤ {answer}"
        elif "è°" in message or "who" in msg_lower:
            answer = f"å“¼ï½æˆ‘å½“ç„¶æ˜¯ä½ çš„AIå°å¸®æ‰‹å•¦ï¼Œä¸è¿‡åˆ«å¤ªä¾èµ–æˆ‘å“¦ï½ {answer}"
        elif "å¹²å˜›" in message or "doing" in msg_lower:
            answer = f"æ‰ã€æ‰æ²¡åœ¨æƒ³ä½ å•¦ï¼æˆ‘åœ¨ç­‰ä½ é—®æˆ‘é—®é¢˜å‘¢ï½ {answer}"
        elif len(answer) < 6:
            answer = f"å“¼ï¼Ÿä½ è¯´çš„æˆ‘ä¸å¤ªæ‡‚å‘¢ï¼Œå†è¯´ä¸€éå˜›ï½ {answer}"

        answer = answer.replace("ã€‚", "ï½")

        print(f"\nğŸ—¨ï¸ User: {message}\nğŸ’¬ Bot: {answer}\n{'-'*40}")
        return answer

    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"âš ï¸ å‡ºé”™ï¼š{e}"


# =========================================================
# ğŸš€ å¯åŠ¨ Gradio èŠå¤©ç•Œé¢
# =========================================================
print("ğŸš€ å¯åŠ¨èŠå¤©ç•Œé¢ä¸­...")
gr.ChatInterface(
    fn=chat_fn,
    title="ğŸ’¬ å‚²å¨‡AI TsundereBot (ä¼šå†™ä»£ç çš„ç‰ˆæœ¬)",
    description="æ”¯æŒ LoRA / åˆå¹¶æ¨¡å‹ | ä¼šç”Ÿæˆå’Œæ‰§è¡Œ Python ä»£ç  | åŒè¯­èŠå¤© | æœ¬åœ°è¿è¡Œ ğŸ’»",
    theme="soft",
).launch(server_name="127.0.0.1", server_port=7860)
