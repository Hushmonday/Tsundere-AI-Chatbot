from datasets import load_dataset
from bitsandbytes.optim import Adam8bit
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForLanguageModeling  # âœ… æ–°å¢
)
import torch

# =========================================================
# ğŸ§  æ¨¡å‹ä¸æ•°æ®é›†åŠ è½½
# =========================================================
model_name = "Qwen/Qwen2.5-1.5B-Instruct"
dataset = load_dataset("json", data_files="data/train.json")

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto",
    low_cpu_mem_usage=True
)
model.gradient_checkpointing_enable()  # âœ… èŠ‚çœä¸€åŠæ¿€æ´»æ˜¾å­˜


# =========================================================
# ğŸ”§ æ•°æ®é¢„å¤„ç†
# =========================================================
def preprocess(example):
    # å°† messages è½¬æˆå•ä¸€å­—ç¬¦ä¸²ï¼Œå¹¶è‡ªåŠ¨åŠ ä¸Šç‰¹æ®Šæ ‡è®°
    text = tokenizer.apply_chat_template(
        example["messages"],
        tokenize=False,
        add_generation_prompt=False
    )
    out = tokenizer(
        text,
        truncation=True,
        max_length=512,   # âœ… ä»1024é™åˆ°512
        padding="max_length"
    )

    out["labels"] = out["input_ids"].copy()
    return out


tokenized = dataset.map(preprocess, remove_columns=dataset["train"].column_names)

# =========================================================
# âš™ï¸ è®­ç»ƒå‚æ•°é…ç½®
# =========================================================
args = TrainingArguments(
    output_dir="./checkpoints",
    num_train_epochs=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,  # âœ… è‡ªåŠ¨æ£€æµ‹æ˜¯å¦æ”¯æŒ CUDA
    logging_steps=10,
    save_strategy="epoch",
    remove_unused_columns=False,

    optim="adamw_bnb_8bit",   # âœ… ç”¨ 8-bit ä¼˜åŒ–å™¨ä»£æ›¿ AdamW
    fp16=False,
    bf16=True,
    per_device_train_batch_size=1,
)

# =========================================================
# ğŸ§© ä¿®å¤ dtype æŠ¥é”™ï¼šå®šä¹‰ DataCollator
# =========================================================
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  # âœ… è‡ªå›å½’è¯­è¨€æ¨¡å‹å¿…é¡»è®¾ä¸º False
)

# =========================================================
# ğŸš€ å¯åŠ¨ Trainer
# =========================================================
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized["train"],
    data_collator=data_collator
)


trainer.train()

# =========================================================
# ğŸ’¾ ä¿å­˜æ¨¡å‹ä¸åˆ†è¯å™¨
# =========================================================
trainer.save_model("./chat_model")
tokenizer.save_pretrained("./chat_model")

print("âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° ./chat_model")
